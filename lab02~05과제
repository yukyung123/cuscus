Lab02
import tensorflow as tf #텐서플로우 실행
tf.enable_eager_execution() #excution 활성화 하여 실행
# Data
x_data = [1, 2, 3, 4, 5]#x데이터/input
y_data = [1, 2, 3, 4, 5]#y데이터/output
# W, b initialize
W = tf.Variable(2.9)#variable로 w값 정의(임의의 값)
b = tf.Variable(0.5)#variable로 b값 정의(임의의 값)

learning_rate = 0.01# learning_rate를 0.01(상수값)으로 지정
for i in range(100+1): # W, b update
#i는 0부터 101까지 101번 update
with tf.GradientTape() as tape: # Gradient descent/ 경사하강 알고리즘
hypothesis = W * x_data + b
cost = tf.reduce_mean(tf.square(hypothesis - y_data)) #가설-실제값의 평균으로 cost지정
W_grad, b_grad = tape.gradient(cost, [W, b]) #변수들에 대한 정보를 테이프에 기록
# gradient는 cost, [W, b]함수에 대해서 변수들에 대한 개별 미분값을 구하여 반환
W.assign_sub(learning_rate * W_grad)# assign sub를 사용하여 w업데이트
b.assign_sub(learning_rate * b_grad)# assign sub를 사용하여 b업데이트
#뺀 값을 다시 그 값에 넣어줌 (a-=b)
#learnin rate는 grad를 얼마만큼 반영할지를 결정함
if i % 10 == 0:
print("{:5}|{:10.4f}|{:10.4}|{:10.6f}".format(i, W.numpy(), b.numpy(), cost))
w와 b, cost값이 얼마만큼 변하는지 중간마다 확인(i값이 10의 배수가 될 때 마다)

lab03
tf.set_random_seed(0) # for reproducibility#다음에 다시 실행했을 때도 똑같이 재현될 수 있도록 random seed를 특정한 값으로 초기화
x_data = [1., 2., 3., 4.]#x값 데이터 지정
y_data = [1., 3., 5., 7.]#y값 데이터 지정
W = tf.Variable(tf.random_normal([1], -100., 100.))
#w정의, 정규분포를 따르는 랜덤 number를 1개로 변수를 만들어 w에 할당
for step in range(300): #gradient descent 300회 진행
hypothesis = W * X #가설 지정
cost = tf.reduce_mean(tf.square(hypothesis - Y))#실제 값과의 차이의 제곱의 평균으로cost 함수 설정
alpha = 0.01 #상수값 지정
gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))# gradient값 지정
descent = W - tf.multiply(alpha, gradient)# alpha값과 gradient식을 곱함
W.assign(descent)
if step % 10 == 0:
print('{:5} | {:10.4f} | {:10.6f}'.format(
step, cost.numpy(), W.numpy()[0]))
#10번에 한번씩 cost값과 w값 출력 
#cost는 0으로 수렴하고 w는 특정 값으로 수렴하는 것을 알 수 있음

Lab04
-다 변수 선형회귀를 어떻게 텐서플로우에서 구현할지-
# data and label
x1 = [ 73., 93., 89., 96., 73.]
x2 = [ 80., 88., 91., 98., 66.]
x3 = [ 75., 93., 90., 100., 70.]#입력 데이터(변수3개)
Y = [152., 185., 180., 196., 142.]#출력, 예측값 데이터,3개의 데이터에 대한 정답
# random weights
w1 = tf.Variable(tf.random_normal([1]))
w2 = tf.Variable(tf.random_normal([1]))
w3 = tf.Variable(tf.random_normal([1]))
b = tf.Variable(tf.random_normal([1]))#tesnsorflow variable로 초기값 1로지정(random 값),변수가 3개이므로 weight도 3개
learning_rate = 0.000001 # learning_rate를0.000001로 지정
for i in range(1000+1): #1001회 시행
# tf.GradientTape() to record the gradient of the cost function
with tf.GradientTape() as tape:# GradientTape에 저장
hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b# hypothesis함수
cost = tf.reduce_mean(tf.square(hypothesis - Y))# hypothesis에서 y(정답데이터)를 뺀 값(=오차) 제곱의 평균값
# calculates the gradients of the cost
w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])#cost 함수에 대한 4개의 변수들에 대한 기울기 값을 구함
# update w1,w2,w3 and b
w1.assign_sub(learning_rate * w1_grad)
w2.assign_sub(learning_rate * w2_grad)
w3.assign_sub(learning_rate * w3_grad)#기울기값 할당, grad값에 learning_rate를 곱하고 빼서 지속적으로 업데이트
b.assign_sub(learning_rate * b_grad)
if i % 50 == 0:
print("{:5} | {:12.4f}".format(i, cost.numpy()))#중간중간50번 마다 cost값을 출력해보도록 함
<결과값>: i값이 0에서 1000으로 쭉 증가 함을 볼 수 있고 cost값은 큰값에서 줄어들고 이제 줄어들지 않는 것을 볼 수 있음

data = np.array([
# X1, X2, X3, y
[ 73., 80., 75., 152. ],
[ 93., 88., 93., 185. ],
[ 89., 91., 90., 180. ],
[ 96., 98., 100., 196. ],
[ 73., 66., 70., 142. ] #matrix가 주어짐
], dtype=np.float32)
# slice data
X = data[:, :-1]#5행1~3열의 데이터[전체,마지막colum 제외],input
y = data[:, [-1]]#5행 마지막열[전체,마지막 colum],정답
W = tf.Variable(tf.random_normal([3, 1]))#x값의 컬럼이 3개이므로 3필요,출력은 1
b = tf.Variable(tf.random_normal([1]))#b이므로 1
learning_rate = 0.000001#작은 상수값 지정
# hypothesis, prediction function
def predict(X):
return tf.matmul(X, W) + b # x*w+b 이후b는 생략 하게 할 수 있음
n_epochs = 2000 #2000번 순회
for i in range(n_epochs+1):  #2001번epochs 수행
# record the gradient of the cost function
with tf.GradientTape() as tape:
cost = tf.reduce_mean((tf.square(predict(X) - y))) #cost 함수 정의
# calculates the gradients of the loss
W_grad, b_grad = tape.gradient(cost, [W, b])
# updates parameters (W and b)
W.assign_sub(learning_rate * W_grad)
b.assign_sub(learning_rate * b_grad)
if i % 100 == 0:
print("{:5} | {:10.4f}".format(i, cost.numpy())) #중간중간100번마다cost 값을 출력해 보도록함

lab05
def grad(hypothesis, labels):
with tf.GradientTape() as tape:
loss_value = loss_fn(hypothesis, labels) #가설과 실제값을 넣음으로서 loss값과 weight 값을 토대로 tape.gradient를통해 값이 바뀌게 됨
return tape.gradient(loss_value, [W,b])
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)# learning_rate값 설정
optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))#주어진 optimizer에 우리가 선언한 함수 grads값과 모델값을 토대로 원하는 최적의 값을 구현

import tensorflow.contrib.eager as tfe#실행시키기위한 library를 불러옴
tf.enable_eager_execution() #execution선언
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#tf데이터를 통해 우리가 원하는 x값과 y값을 실제 x의 길이만큼 batch 하겠다는 것을 의미함
W = tf.Variable(tf.zeros([2,1]), name='weight')#모델의 모양과 이름 지정
b = tf.Variable(tf.zeros([1]), name='bias')
def logistic_regression(features):
hypothesis = tf.div(1., 1. + tf.exp(tf.matmul(features, W) + b))#리니어한 값을 시그모이드 함수에 넣음
return hypothesis # hypothesis가 나옴
def loss_fn(hypothesis, labels): # hypothesis값을 labals에 넣으면
cost = -tf.reduce_mean(labels * tf.log(loss_fn(hypothesis) + (1 - labels) * tf.log(1 - hypothesis))
return cost
def grad(hypothesis, features, labels): 
with tf.GradientTape() as tape:
loss_value = loss_fn(hypothesis,labels) #hypothesis, labels값이 가설과 실제를 비교한 loss값을 구할 수 있음
return tape.gradient(loss_value, [W,b]) #gradient를 통해 우리의 모델값을 계속 바꿔감
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)#learning_rate=0.01을입력후 Optimizer선언
for step in range(EPOCHS):# 루프를 걸면서 함수를 호출하는 형태, 함수를 epochs만큼 돌려봄
for features, labels in tfe.Iterator(dataset):#앞에서 지정한 데이터 값을 불러 온후 데이터를 토대로 Iterator 실제 x값과 y값을 넣어가며 모델이 만들어짐
grads = grad(logistic_regression(features), features, labels)
optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))#grade 값을 minimize,이과정을 통해 w와 b가 업데이트됨(최적의 값)
if step % 100 == 0:
print("Iter: {}, Loss: {:.4f}".format(step, loss_fn(logistic_regression(features) ,labels))) #iter와 loss값이 줄어드는 것을 출력하기 위한 값
def accuracy_fn(hypothesis, labels):#가설과 함수를 비교하기위함
predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) #decision boundury를 결정하기 위한 구간, 시그모이드의 1과 0사이에 나온 0.5를 통해 예측된 값이 나오게 됨
accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))#실제값과 예측값을 비교했을때 값이 맞는지 accuracy를 출력하여 확인
return accuracy
test_acc = accuracy_fn(logistic_regression(x_test),y_test)#x테스트와 y테스트를 넣어 확인
